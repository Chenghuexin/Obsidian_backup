
# Abastract
+ survival analysis model for NPC (malignant tumors)
+ nutritional status of cancer patients is closely associated with the clinical progression of the disease.
+ combine TabNet and Cox models
+ Using:
	+ TabNet: sequential attention mechanism
		+ extract interpretable features for identifying disease risk factors
	+ Cox: ?

# Motivation
+ the complexity of neural networks and their incompatibility with medical tabular data can reduce the interpretability of the model

# Introduction
### Malnutrition 
A significant factor in the occurrence and development of tumors, affecting the entire course of cancer and serving as the primary negative factor for the adverse clinical outcomes of tumor patients

Previous studies have shown that approximately 40% of patients with malignant tumors die from malnutrition, not from the disease itself

Most patients with malignant tumors require nutritional intervention. Therefore, timely and accurate detection of malnutrition in patients with nasopharyngeal carcinoma

### Survival analysis model
+ time-to-event model
+ in medical field, it also can identify the risk factors that impact patient survival
	+ for early decision-making
	+ improved treatment strategies
##### Cox Proportional Hazard Model (CPH)
Drawback
+ it posits that the logarithmic survival risk of a patient can be expressed as a linear combination of patient covariates (預設病因是線性關係), which is considered overly simplistic for many practical medical situations due to the frequent presence of complex nonlinear relationships among variables.

### Deep Survival Analysis model
+ Overcome the linear constraints of CPH
	+ combine DNN and CPH
Challenge
+ many neural networks have not shown comparable levels of effectiveness in handling tabular data
	+ In practice, clinical survival data frequently comprises a significant amount of tabular data, which many survival analysis models fail to adequately consider
	+ tabular data:
		+ Tabular Data Characteristics
			+ 特徵異質（heterogeneous features）
			- 樣本數量相對較小
			- 可能會有 curse of dimension
			- 含有極端值與離群值（outliers）
		- 深度學習架構的限制
			- 主要為連續數值資料設計 (影像、語音)
			- 無法有效處理類別型或非線性邏輯複雜的 tabular 資料
+ the interpretability

# Tab-Cox
### Input
input data enters the multi-step decision of the model after proceeding through the **BN layer**

### Decision Step
+ relies on sequential multi-step processing with N-steps decision
+ the input of each decision step is affected by information from the previous step to determine which features to utilize
+ At each decision step, the processed feature representation will be outputted along with the single-step prediction vector
+ Finally, the feature representation is aggregated to calculate the global feature importance.
+ At each decision step, the model adjusts its focus by considering past outcomes and current information.
+ 2 pivotal components:
	1. Attentive Transformer: feature selection
	2. Feature Transformer: feature extraction
### Attentive Transformer
+ generate a feature mask that assesses the importance of each feature
+ “Instance-Wise” feature selection
	+ 不同資料其相同的特徵權重會不一樣
+ generate a feature mask
	+ 用來在模型訓練過程中**選擇性地關注輸入資料中的某些特徵欄位**，忽略其他特徵，讓模型能夠專注在對預測最有貢獻的欄位上
	+ 一個與輸入特徵同樣維度的權重向量，通常數值介於 0 到 1 之間
	+ decisions made at each step will not be wasted on irrelevant features
![[Pasted image 20250614234930.png]]
+ Step 1 and 2: the output of the Feature transformer of the previous decision step will be input to the split layerr, which will split it and provide input to the input of the Attentive Transformer of the current decision step $a[i-1]$
+ Step 3: $hi$ Layer, consists of the Fully Connected Layer (FC) and Batch Normalization Layer (BN) to extract higher-dimensional and more abstract features
+ Step 4 and 5: determine the feature weight
	+ sparsemax activation function: $M [i] = sparsemax (P [i − 1] · hi (a [i − 1]))$
+ Step 6: $P[i] = \prod_{j=1}^{i} (\gamma - M[j])$
	+ **越早被選過的特徵，在後面就越不容易再被選中**（透過連乘，權重會下降）
	+ 是一種**特徵使用抑制機制**，鼓勵模型探索其他特徵
+ Step 7: $M[i]$ generated by the Attentive Transformer will be integrated with the original features and input into the subsequent Feature Transformer

### Feature Transformer
![[Pasted image 20250615042223.png]]

Feature Block: consist of sequentially connected FC, BN, and GLU layers
+ GLU: Gated Linear Unit, which implements selective input filtering to help the network capture long-term dependency information
	+ $h_l(X) = (X \cdot W + b) \otimes \sigma(X \cdot V + c)$
		+ $X$: input feature
		+ $W, V$: weight matrix
		+ $b,c$: bias
		+ $\sigma$: Sigmoid (output 0 ~ 1)
		+ $\otimes$: element-wise multiplication

Main Function: 
+ extract the features of each decision step
+ split this features into **shared decision part** and the **independent decision part**

Shared Decision Part: shares parameters in all decision steps, allowing for training all decision steps simultaneously and calculating common features
+ 有助於模型更細緻地擷取資料的異質性資訊

Independent Decision Part:
The parameters are obtained only by training each decision step separately, and the characteristic features are calculated

Step 3: $[d[i],a[i]]=Split(FeatureTransformer(M[i]⋅f))$
+ $d[i]$: input to the ReLu layer and combines the result of each step’s decision into the total decision $d_{out}$:
	+ $d_{out} = \sum_{i=1}^{N_{steps}} \text{ReLU}(d[i])$
+ $a[i]$: input to the Attentive Transformer for the next step

### Survival Risk Loss

##### Primary purpose of survival analysis
examine the relationship between variable x and the survival risk function
+ risk function $h(t,x)$: instantaneous mortality rate of patients at a specific time ($x$)

##### Cox Proportional Hazards Model
Formula: $h(t,x)=h_0​(t)⋅exp[g(x)]$
+ $g(x)=β^Tx$
- $h(t,x)$: 某人在時間 $t$ 的死亡風險。
    
- $h0(t)$: 基準風險函數（baseline hazard）——不含個人特徵的死亡率。
    
- $g(x)=β^Tx$: 表示特徵 $x$ 對風險的線性影響，$β$ 是回歸係數向量。

Converted formula: $ln(h(t,x)/h_0(t))$
