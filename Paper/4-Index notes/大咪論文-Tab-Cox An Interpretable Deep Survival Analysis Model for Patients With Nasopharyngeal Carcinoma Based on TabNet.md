
# Abastract
+ survival analysis model for NPC (malignant tumors)
+ nutritional status of cancer patients is closely associated with the clinical progression of the disease.
+ combine TabNet and Cox models
+ Using:
	+ TabNet: sequential attention mechanism
		+ extract interpretable features for identifying disease risk factors
	+ Cox: ?

# Motivation
+ the complexity of neural networks and their incompatibility with medical tabular data can reduce the interpretability of the model

# Introduction
### Malnutrition 
A significant factor in the occurrence and development of tumors, affecting the entire course of cancer and serving as the primary negative factor for the adverse clinical outcomes of tumor patients

Previous studies have shown that approximately 40% of patients with malignant tumors die from malnutrition, not from the disease itself

Most patients with malignant tumors require nutritional intervention. Therefore, timely and accurate detection of malnutrition in patients with nasopharyngeal carcinoma

### Survival analysis model
+ time-to-event model
+ in medical field, it also can identify the risk factors that impact patient survival
	+ for early decision-making
	+ improved treatment strategies
##### Cox Proportional Hazard Model (CPH)
Drawback
+ it posits that the logarithmic survival risk of a patient can be expressed as a linear combination of patient covariates (預設病因是線性關係), which is considered overly simplistic for many practical medical situations due to the frequent presence of complex nonlinear relationships among variables.

### Deep Survival Analysis model
+ Overcome the linear constraints of CPH
	+ combine DNN and CPH
Challenge
+ many neural networks have not shown comparable levels of effectiveness in handling tabular data
	+ In practice, clinical survival data frequently comprises a significant amount of tabular data, which many survival analysis models fail to adequately consider
	+ tabular data:
		+ Tabular Data Characteristics
			+ 特徵異質（heterogeneous features）
			- 樣本數量相對較小
			- 可能會有 curse of dimension
			- 含有極端值與離群值（outliers）
		- 深度學習架構的限制
			- 主要為連續數值資料設計 (影像、語音)
			- 無法有效處理類別型或非線性邏輯複雜的 tabular 資料
+ the interpretability

# Tab-Cox
### Input
input data enters the multi-step decision of the model after proceeding through the **BN layer**

### Decision Step
+ relies on sequential multi-step processing with N-steps decision
+ the input of each decision step is affected by information from the previous step to determine which features to utilize
+ At each decision step, the processed feature representation will be outputted along with the single-step prediction vector
+ Finally, the feature representation is aggregated to calculate the global feature importance.
+ At each decision step, the model adjusts its focus by considering past outcomes and current information.
+ 2 pivotal components:
	1. Attentive Transformer: feature selection
	2. Feature Transformer: feature extraction
### Attentive Transformer
+ generate a feature mask that assesses the importance of each feature
+ “Instance-Wise” feature selection
	+ 不同資料其相同的特徵權重會不一樣
+ generate a feature mask
	+ 用來在模型訓練過程中**選擇性地關注輸入資料中的某些特徵欄位**，忽略其他特徵，讓模型能夠專注在對預測最有貢獻的欄位上
	+ 一個與輸入特徵同樣維度的權重向量，通常數值介於 0 到 1 之間
	+ decisions made at each step will not be wasted on irrelevant features
![[Pasted image 20250614234930.png]]
+ Step 1 and 2: the output of the Feature transformer of the previous decision step will be input to the split layerr, which will split it and provide input to the input of the Attentive Transformer of the current decision step $a[i-1]$
+ Step 3: $hi$ Layer, consists of the Fully Connected Layer (FC) and Batch Normalization Layer (BN) to extract higher-dimensional and more abstract features
+ Step 4 and 5: determine the feature weight
	+ sparsemax activation function: $M [i] = sparsemax (P [i − 1] · hi (a [i − 1]))$
+ Step 6: $P[i] = \prod_{j=1}^{i} (\gamma - M[j])$
	+ **越早被選過的特徵，在後面就越不容易再被選中**（透過連乘，權重會下降）
	+ 是一種**特徵使用抑制機制**，鼓勵模型探索其他特徵
+ Step 7: $M[i]$ generated by the Attentive Transformer will be integrated with the original features and input into the subsequent Feature Transformer

### Feature Transformer
![[Pasted image 20250615042223.png]]

Feature Block: consist of sequentially connected FC, BN, and GLU layers
+ GLU: Gated Linear Unit, which implements selective input filtering to help the network capture long-term dependency information
	+ $h_l(X) = (X \cdot W + b) \otimes \sigma(X \cdot V + c)$
		+ $X$: input feature
		+ $W, V$: weight matrix
		+ $b,c$: bias
		+ $\sigma$: Sigmoid (output 0 ~ 1)
		+ $\otimes$: element-wise multiplication

Main Function: 
+ extract the features of each decision step
+ split this features into **shared decision part** and the **independent decision part**

Shared Decision Part: shares parameters in all decision steps, allowing for training all decision steps simultaneously and calculating common features
+ 有助於模型更細緻地擷取資料的異質性資訊

Independent Decision Part:
The parameters are obtained only by training each decision step separately, and the characteristic features are calculated

Step 3: $[d[i],a[i]]=Split(FeatureTransformer(M[i]⋅f))$
+ $d[i]$: input to the ReLu layer and combines the result of each step’s decision into the total decision $d_{out}$:
	+ $d_{out} = \sum_{i=1}^{N_{steps}} \text{ReLU}(d[i])$
+ $a[i]$: input to the Attentive Transformer for the next step

### Survival Risk Loss

##### Primary purpose of survival analysis
examine the relationship between variable x and the survival risk function
+ risk function $h(t,x)$: instantaneous mortality rate of patients at a specific time ($x$)

##### Cox Proportional Hazards Model
Formula: $h(t,x)=h_0​(t)⋅exp[g(x)]$
+ $g(x)=β^Tx$
- $h(t,x)$: 某人在時間 $t$ 的死亡風險。
    
- $h0(t)$: 基準風險函數（baseline hazard）——不含個人特徵的死亡率。
    
- $g(x)=β^Tx$: 表示特徵 $x$ 對風險的線性影響，$β$ 是回歸係數向量。

Converted formula: $ln(\frac{h(t,x)}{h_0(t)})=β^Tx$

Strong assumnption is unrealistic:
+ each risk factor remains constant over time
+ risk factor exhibits a linear relationship with the logarithm of the risk ratio

##### Improve Cox - 1
+ Utilizes the output of the neural network to replace $g(x)$
+ Formula: $Loss=\sum_{i}D_i​⋅log​(\sum_{j∈R_i}​exp[g(x_j​)−g(x_i​)])​$
	+ What is $R_i$ (風險集) : 在時間 $t_i$（即第 i 個樣本事件發生的時間）仍然存活或尚未發生事件的所有其他樣本的集合
+ Batch-friendly: $batchLoss=\frac{1}{n}\sum_{i:D_i=1}\log​(\sum_{j∈\tilde{R_i}}​exp[g(x_j​)−g(x_i​)])$
	+ $\tilde{R_i}$: 在 batch 中仍存活的風險集（非整體資料集）
+ Limitation: 雖然解決了線性假設，但 Cox 的另一個假設「風險比例恆定（時間無關）」仍未被處理。

##### Improve Cox - 2
+ $g(x)\Rightarrow g(t,x)$, time is also fitted as a covariate
	+ $g(t,x)=x\log t$
+ Risk function after improving: $h (t|x) = h_0 (t) exp [g (t, x)]$
+ After improving loss function: $TLoss=\frac{1}{n}\sum_{i:C_i=1}\log​(\sum_{j∈\tilde{R_i}}​exp[g(T_i, x_j​)−g(T_i, x_i)])$

In order to further control the sparsity of the selected features
+ $M\_Loss = \frac{1}{N_{\text{steps}} \cdot B} \sum_{i=1}^{N_{\text{steps}}} \sum_{b=1}^{B} \sum_{j=1}^{D}-M_{b,j}^{[i]} \cdot \log(M_{b,j}^{[i]} + \epsilon)$
	+ $\epsilon$：防止 log(0) 的極小值常數。
	+ constrain the weight of $M$, the smaller the M_Loss, the sparser the attention mask $M[j]$ becomes
	+ 只挑選出少量的關鍵特徵（提高 interpretability、減少過擬合）

Final Loss Function: T_Loss + (λ∗M_Loss)
+ λ：控制兩者之間的權重比例，超參數

### Explainable Risk Factor Evaluation
The feature mask can only be analyzed as the local variable importance. However, the analysis of global variable importance requires combining the local variable importance from different steps

The total decision contribution of sample b in the i step:
$\eta_b[i]=\sum_{c=1}^{N_d}ReLU(d_{b,c}[i])$
+ larger $d_{b,c}[i]$, greater importance

The final global variable importance is determined by combining the contribution of a single decision step $ηb[i]$ and the local variable importance analysis $M_{b,j}[i]$
+ formula: $M_{\text{agg}}[j] = \frac{\sum_{i=1}^{N_{\text{steps}}} \eta_b[i] \cdot M_{b,j}[i]}{\sum_{j=1}^{D} \sum_{i=1}^{N_{\text{steps}}} \eta_b[i] \cdot M_{b,j}[i]}$

# Experiment
### Evaluation Metric
##### C-index
only considers the ability to predict whether an event occurs or not, and does not take the time factor into account. Since time factor is a crucial factor in the proposed model

##### time-dependent Cindex
+ formula: $C_{td} = P\left( \hat{S}(T_i \mid x_i) < \hat{S}(T_i \mid x_j) \,\middle|\, T_i < T_j,\ D_i = 1 \right)$
	- $\hat{S}(T_i \mid x_i)$：樣本 i 在時間 $T_i$​ 的生存機率預測（生存函數的估計）。
    
	- $T_i < T_j$​：樣本 i 發生事件的時間早於樣本 j。
    
	- $D_i = 1$：表示樣本 i 是「有觀察到事件發生」（非截尾資料）。
    
	- $P(\cdot)$：表示滿足條件時，預測的生存概率正確排序的比例。
- a higher C index indicates a stronger ability of the model
### DataSet